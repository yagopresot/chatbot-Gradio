# -*- coding: utf-8 -*-
"""Chatbot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j5d4oRpg5EOwymhKSHiM6Qm4nCHaf_lZ
"""

!pip install litellm

from google.colab import userdata
Groq_API_Key = userdata.get('Groq_API_Key')

from litellm import completion

messages = [
    {"role": "system", "content": "Voc√™ √© o chat da Terra e do Universo, que responde perguntas em portugu√™s brasileiro sobre previs√£o do tempo na Terra e no espa√ßo pr√≥ximo √† Terra, al√©m de informa√ß√µes sobre terremotos."},
    {"role": "user", "content": "Qual √© a frequ√™ncia dos m√°ximos solares?"}
]

response = completion(
    model = "groq/gemma2-9b-it",
    messages=messages,
    api_key=Groq_API_Key
)

print(response.choices[0].message.content)

response = completion(
    model = "groq/llama-3.3-70b-versatile",
    messages=messages,
    api_key=Groq_API_Key
)

print(response.choices[0].message.content)

def call_groq_api(messages, model="groq/llama-3.3-70b-versatile"):
  response = completion(
      model = model,
      messages = messages,
      api_key = Groq_API_Key
  )
  return response.choices[0].message.content

def chat():
  print("Iniciando chat com o modelo. Digite sair para encerrar.")
  messages = [
       {"role": "system", "content": "Voc√™ √© o chat da Terra e do Universo, que responde perguntas em portugu√™s brasileiro sobre previs√£o do tempo na Terra e no espa√ßo pr√≥ximo √† Terra, al√©m de informa√ß√µes sobre terremotos."},
  ]

  while True:
    user_massage = input("Voc√™: ")
    if user_massage.lower() == "sair":
      break
    messages.append({"role": "user", "content": user_massage})
    model_response = call_groq_api(messages)
    messages.append({"role": "assistant", "content": model_response})
    print(f"Assistente: {model_response}")

chat()

import requests
import json

def previsao_do_tempo(city, country):
    Wheater_API = userdata.get('Wheater_API')
    url = f'http://api.openweathermap.org/data/2.5/weather?q={city},{country}&APPID={Wheater_API}&lang=pt_br&units=metric'
    response = requests.get(url)
    data = response.json()
    return json.dumps(data)

previsao_do_tempo('S√£o Paulo', 'BR')

tools=[
    {
        "type": "function",
        "function": {
            "name": "previsao_do_tempo",
            "description": "Retorna a previs√£o do tempo para a cidade e pa√≠s especificados",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "Nome da cidade"
                    },
                    "country": {
                        "type": "string",
                        "description": "Sigla do pa√≠s"
                    }
                },
                "required": ["city", "country"]
            }
        }
    }
]

# Fun√ß√£o para chamar a API com o hist√≥rico de mensagens
def call_groq_api(messages, model="groq/llama-3.3-70b-versatile"):
    global tools
    response = completion(
        model=model,
        messages=messages,
        tools=tools,
        tool_choice="auto",
        api_key=Groq_API_Key,
    )
    resposta_texto = response.choices[0].message
    chamada_ferramentas = resposta_texto.tool_calls
    if chamada_ferramentas:
      available_functions = {
        "previsao_do_tempo": previsao_do_tempo,
      }
      for tool_call in chamada_ferramentas:
        function_name = tool_call.function.name
        function_to_call = available_functions[function_name]
        function_args = json.loads(tool_call.function.arguments)
        function_response = function_to_call(
            city=function_args.get("city"),
            country=function_args.get("country"),
        )
        return function_response

    else:
      return resposta_texto.content

chat()

def verificar_tempestade_solar():
    url = "https://services.swpc.noaa.gov/products/noaa-planetary-k-index.json"
    responde = requests.get(url)
    if responde.status_code == 200: # Corrected variable name and attribute
      data = responde.json() # Corrected variable name
      latest_kp = float(data[-1][1])
      if latest_kp >= 5:
        return f"Alerta de Tempestade solar! √çndice Kp atual: {latest_kp}" # Corrected variable name
      else:
        return f"Sem alertas de tempestade solar. √çndice Kp atual: {latest_kp}" # Corrected variable name
    else:
      return f"Erro ao obter dados da tempestade solar. C√≥digo de status: {responde.status_code}" # Added error handling

tools = [
        {
            "type": "function",
            "function": {
                "name": "previsao_do_tempo",
                "description": "Retorna a previs√£o do tempo em uma cidade espec√≠fica",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {
                            "type": "string",
                            "description": "Nome da cidade",
                        },
                        "country": {
                            "type": "string",
                            "description": "Sigla do pa√≠s",
                        },
                    },
                    "required": ["city", "country"],
                },
            }

        },

        {
        "type": "function",
        "function": {
            "name": "verificar_tempestade_solar",
            "description": "Verifica se h√° uma tempestade solar em andamento",
            "parameters": {
                "type": "object",
                "properties": {},
                "required": [],
            },
        }
    }

]

# Fun√ß√£o para chamar a API com o hist√≥rico de mensagens
def call_groq_api(messages, model="groq/llama-3.3-70b-versatile"):
    global tools
    response = completion(
        model=model,
        messages=messages,
        tools=tools,
        tool_choice="auto",
        api_key=Groq_API_Key,
    )
    resposta_texto = response.choices[0].message
    chamada_ferramentas = resposta_texto.tool_calls
    if chamada_ferramentas:
        available_functions = {
            "previsao_do_tempo": previsao_do_tempo,
            "verificar_tempestade_solar": verificar_tempestade_solar,
        }
        for tool_call in chamada_ferramentas:
            function_name = tool_call.function.name
            function_to_call = available_functions[function_name]
            function_args = json.loads(tool_call.function.arguments)

            match function_name:
                case "previsao_do_tempo":
                    function_response = function_to_call(
                        city=function_args.get("city"),
                        country=function_args.get("country"),
                    )
                case "verificar_tempestade_solar":
                    function_response = function_to_call()
            return function_response

    else:
        return resposta_texto.content

chat()

import pandas as pd

def extrair_sismos():
    url = 'https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5_day.csv'

    df = pd.read_csv(url)
    return df

extrair_sismos()

tools = [
        {
            "type": "function",
            "function": {
                "name": "previsao_do_tempo",
                "description": "Retorna a previs√£o do tempo em uma cidade espec√≠fica",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {
                            "type": "string",
                            "description": "Nome da cidade",
                        },
                        "country": {
                            "type": "string",
                            "description": "Sigla do pa√≠s",
                        },
                    },
                    "required": ["city", "country"],
                },
            }

        },

      {
        "type": "function",
        "function": {
            "name": "verificar_tempestade_solar",
            "description": "Verifica se h√° uma tempestade solar em andamento",
            "parameters": {
                "type": "object",
                "properties": {},
                "required": [],
            },
        }
      },
      {
        "type": "function",
        "function": {
            "name": "extrair_sismos",
            "description": "Extrai dados de sismos da USGS",
            "parameters": {
                "type": "object",
                "properties": {},
                "required": [],
            },
        }
      }
]

# Fun√ß√£o para chamar a API com o hist√≥rico de mensagens
def call_groq_api(messages, model="groq/llama-3.3-70b-versatile"):
    global tools
    response = completion(
        model=model,
        messages=messages,
        tools=tools,
        tool_choice="auto",
        api_key=Groq_API_Key,
    )
    resposta_texto = response.choices[0].message
    chamada_ferramentas = resposta_texto.tool_calls
    if chamada_ferramentas:
      available_functions = {
        "previsao_do_tempo": previsao_do_tempo,
        "verificar_tempestade_solar": verificar_tempestade_solar,
        "extrair_sismos": extrair_sismos
      }
      for tool_call in chamada_ferramentas:
        function_name = tool_call.function.name
        function_to_call = available_functions[function_name]
        function_args = json.loads(tool_call.function.arguments)

        match function_name:
          case "previsao_do_tempo":

            function_response = function_to_call(
                city=function_args.get("city"),
                country=function_args.get("country"),
            )
          case "verificar_tempestade_solar":
            function_response = function_to_call()
          case "extrair_sismos":
            function_response = function_to_call()
        return function_response

    else:
      return resposta_texto.content

# Fun√ß√£o para iniciar o chat, mantendo o hist√≥rico
def chat():
    print("Iniciando chat com o modelo. Digite 'sair' para encerrar.")

    # Hist√≥rico de mensagens
    messages = [{"role": "system", "content": """
    Voc√™ √© o Chat da Terra e do Universo e responde em portugu√™s brasileiro
    perguntas sobre a previs√£o do tempo na Terra e do espa√ßo pr√≥ximo √† Terra, al√©m de informa√ß√µes sobre terremotos.
    """}]

    while True:
        user_message = input("Voc√™: ")
        if user_message.lower() == "sair":
            print("Encerrando chat. At√© a pr√≥xima!")
            break

        # Adicionar a mensagem do usu√°rio ao hist√≥rico
        messages.append({"role": "user", "content": user_message})

        # Chamar a API com o hist√≥rico completo
        model_response = call_groq_api(messages)

        # Exibir a resposta do assistente
        display(model_response)
        if isinstance(model_response, pd.DataFrame):
            print("O model_response √© um DataFrame.")
            texto_corrido = ""
            for index, row in model_response.iterrows():
                texto_corrido += f"Evento {index + 1}: Magnitude {row['mag']}, Local {row['place']}, Tempo {row['time']}\n"

            model_response = texto_corrido


        # Adicionar a resposta do modelo ao hist√≥rico
        messages.append({"role": "assistant", "content": model_response})

chat()

!pip install pinecone

Pinecone_API = userdata.get('Pinecone_API')

from pinecone import Pinecone

pc = Pinecone(api_key=Pinecone_API)

data = [
    {
        "id": "occurrence1",
        "text": "Ouro presente em veios de quartzo em uma forma√ß√£o hidrotermal. Observa-se alta concentra√ß√£o de ouro em zonas de fraturas e falhas."
    },
    {
        "id": "occurrence2",
        "text": "Associa√ß√£o do ouro com sulfetos, especialmente pirita e arsenopirita, em ambiente de rochas metavulc√¢nicas. Indica potencial para dep√≥sito orog√™nico de ouro."
    },
    {
        "id": "occurrence3",
        "text": "Ouro aluvial encontrado em dep√≥sitos de cascalho pr√≥ximo a rios e c√≥rregos. Indica transporte e concentra√ß√£o secund√°ria de ouro."
    },
    {
        "id": "occurrence4",
        "text": "Presen√ßa de ouro em forma√ß√µes de skarn associadas a intrus√µes √≠gneas gran√≠ticas. Indica forma√ß√£o relacionada a processos de metamorfismo de contato."
    },
    {
        "id": "occurrence5",
        "text": "Ouro disseminado em forma√ß√µes sedimentares de origem marinha, em conglomerados ricos em minerais pesados. Potencial para dep√≥sito do tipo placer ou paleoplacer."
    },
]

index = pc.Index("geologia")

embeddings = pc.inference.embed(
    "llama-text-embed-v2",
    inputs=[d['text'] for d in data],
    parameters={
        "input_type": "passage"
    }
)

vectors = []
for d, e in zip(data, embeddings):
    vectors.append({
        "id": d['id'],
        "values": e['values'],
        "metadata": {'text': d['text']}
    })

index.upsert(vectors=vectors,namespace="ns1")

query = "o ouro ocorre em sedimentos de origem marinha?"

x = pc.inference.embed(
    model="llama-text-embed-v2",
    inputs=[query],
    parameters={
        "input_type": "query"
    }
)

results = index.query(
    namespace="ns1",
    top_k=3,
    include_values=False,
    include_metadata=True,
    vector=x[0].values,
)

results

!pip install pypdf

import pypdf

from google.colab import drive
drive.mount('/content/drive')

def chunk_text(text, chunk_size=500):
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

with open("/content/drive/MyDrive/Projetos/dados/livro.pdf", "rb") as file:
    reader = pypdf.PdfReader(file)
    text = ""

    for page_num in range(len(reader.pages)):
        page = reader.pages[page_num]
        text += page.extract_text()

chunks = chunk_text(text)

chunks[11]

data = []

for i, chunk in enumerate(chunks[:90], start=1):
    data.append({
        "id": f"chunk{i}",
        "text": chunk.strip()
    })

for chunk in data[:5]:
    print(chunk)

embeddings = pc.inference.embed(
    "llama-text-embed-v2",
    inputs=[d['text'] for d in data],
    parameters={
        "input_type": "passage"
    }
)

vectors = []
for d, e in zip(data, embeddings):
    vectors.append({
        "id": d['id'],
        "values": e['values'],
        "metadata": {'text': d['text']}
    })

index.upsert(
    vectors=vectors,
    namespace="ns1"
)

def info_geologia(query):
    x = pc.inference.embed(
        model="llama-text-embed-v2",
        inputs=[query],
        parameters={
            "input_type": "query"
        }
    )
    results = index.query(
        namespace="ns1",
        vector=x[0].values,
        top_k=3,
        include_values=False,
        include_metadata=True
    )
    return results

query = "o ouro ocorre em sedimentos de origem marinha?"
info_geologia(query)

resposta = info_geologia(query)
resposta.matches[0].metadata['text']

# Define user_message with a placeholder value to avoid NameError
user_message = "This is a placeholder message." # Placeholder definition

# Verificar se o tema √© geologia
if "geologia" in user_message.lower():
        # Chamar a fun√ß√£o info_geologia para obter informa√ß√µes adicionais
        resposta = info_geologia(user_message)

# Adicionar as informa√ß√µes de geologia ao hist√≥rico como contexto extra
geologia_info = f"Informa√ß√µes adicionais sobre geologia: {resposta.matches[0].metadata['text']}"
messages.append({"role": "system", "content": geologia_info})

# Fun√ß√£o para iniciar o chat, mantendo o hist√≥rico
def chat():
    print("Iniciando chat com o modelo. Digite 'sair' para encerrar.")

    # Hist√≥rico de mensagens
    messages = [{"role": "system", "content": """
    Voc√™ √© o Chat da Terra e do Universo e responde em portugu√™s brasileiro
    perguntas sobre a previs√£o do tempo na Terra e do espa√ßo pr√≥ximo √† Terra, al√©m de informa√ß√µes sobre terremotos.
    """}]

    while True:
        user_message = input("Voc√™: ")
        if user_message.lower() == "sair":
            print("Encerrando chat. At√© a pr√≥xima!")
            break

        # Adicionar a mensagem do usu√°rio ao hist√≥rico
        messages.append({"role": "user", "content": user_message})

        # Verificar se o tema √© geologia
        if "geologia" in user_message.lower():
            # Chamar a fun√ß√£o info_geologia para obter informa√ß√µes adicionais
            resposta = info_geologia(user_message)

            # Adicionar as informa√ß√µes de geologia ao hist√≥rico como contexto extra
            geologia_info = f"Informa√ß√µes adicionais sobre geologia: {resposta.matches[0].metadata['text']}"
            messages.append({"role": "system", "content": geologia_info})

        # Chamar a API com o hist√≥rico completo
        model_response = call_groq_api(messages)

        # Exibir a resposta do assistente
        display(model_response)
        if isinstance(model_response, pd.DataFrame):
            print("O model_response √© um DataFrame.")
            texto_corrido = ""
            for index, row in model_response.iterrows():
                texto_corrido += f"Evento {index + 1}: Magnitude {row['mag']}, Local {row['place']}, Tempo {row['time']}\n"

            model_response = texto_corrido

        # Adicionar a resposta do modelo ao hist√≥rico
        messages.append({"role": "assistant", "content": model_response})

chat()

!pip install litellm gradio pinecone

from litellm import completion
import requests
import json
import gradio as gr
import pandas as pd
from pinecone import Pinecone
import os
from google.colab import userdata

def info_geologia(query):
    PINECONE_API = userdata.get('PINECONE_API')
    pc = Pinecone(api_key=PINECONE_API)
    index = pc.Index("geologia")
    x = pc.inference.embed(
        model="llama-text-embed-v2",
        inputs=[query],
        parameters={
            "input_type": "query"
        }
    )
    results = index.query(
        namespace="ns1",
        vector=x[0].values,
        top_k=3,
        include_values=False,
        include_metadata=True
    )
    return results

def previsao_do_tempo(city, country):
    WEATHER_API = userdata.get('Wheater_API')
    url = f"http://api.openweathermap.org/data/2.5/weather?q={city},{country}&APPID={WEATHER_API}&lang=pt_br&units=metric" # Corrected variable name
    response = requests.get(url)
    data = response.json()

    return json.dumps(data)


def verificar_tempestade_solar():
    url = "https://services.swpc.noaa.gov/products/noaa-planetary-k-index.json"
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        latest_kp = float(data[-1][1])  # O √∫ltimo valor Kp
        if latest_kp >= 5:
            return f"Alerta de tempestade solar! √çndice Kp atual: {latest_kp}"
        else:
            return f"Sem tempestade solar no momento. √çndice Kp atual: {latest_kp}"
    else:
        return "N√£o foi poss√≠vel obter informa√ß√µes sobre tempestades solares no momento."

def extrair_sismos():
    # Fazer a requisi√ß√£o para obter o conte√∫do da p√°gina
    url = 'https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5_day.csv'

    df = pd.read_csv(url)

    # Retornar o DataFrame com os dados
    return df


tools = [
        {
            "type": "function",
            "function": {
                "name": "previsao_do_tempo",
                "description": "Retorna a previs√£o do tempo em uma cidade espec√≠fica",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {
                            "type": "string",
                            "description": "Nome da cidade",
                        },
                        "country": {
                            "type": "string",
                            "description": "Sigla do pa√≠s",
                        },
                    },
                    "required": ["city", "country"],
                },
            }

        },

      {
        "type": "function",
        "function": {
            "name": "verificar_tempestade_solar",
            "description": "Verifica se h√° uma tempestade solar em andamento",
            "parameters": {
                "type": "object",
                "properties": {},
                "required": [],
            },
        }
      },
      {
        "type": "function",
        "function": {
            "name": "extrair_sismos",
            "description": "Extrai dados de sismos da USGS",
            "parameters": {
                "type": "object",
                "properties": {},
                "required": [],
            },
        }
      }
]


# Fun√ß√£o para chamar a API com o hist√≥rico de mensagens
def call_groq_api(messages, model="groq/llama-3.3-70b-versatile"):
    global tools
    GROQ_API_KEY = userdata.get('Groq_API_Key')
    response = completion(
        model=model,
        messages=messages,
        tools=tools,
        tool_choice="auto",
        api_key=GROQ_API_KEY,
    )
    resposta_texto = response.choices[0].message
    chamada_ferramentas = resposta_texto.tool_calls
    if chamada_ferramentas:
      available_functions = {
        "previsao_do_tempo": previsao_do_tempo,
        "verificar_tempestade_solar": verificar_tempestade_solar,
        "extrair_sismos": extrair_sismos
      }
      for tool_call in chamada_ferramentas:
        function_name = tool_call.function.name
        function_to_call = available_functions[function_name]
        function_args = json.loads(tool_call.function.arguments)

        match function_name:
          case "previsao_do_tempo":

            function_response = function_to_call(
                city=function_args.get("city"),
                country=function_args.get("country"),
            )
          case "verificar_tempestade_solar":
            function_response = function_to_call()
          case "extrair_sismos":
            function_response = function_to_call()
        return function_response

    else:
      return resposta_texto.content

def response(message, history):
    messages = [{"role": "system", "content": """
    Voc√™ √© o Chat da Terra e do Universo e responde em portugu√™s brasileiro
    perguntas sobre a previs√£o do tempo na Terra e do espa√ßo pr√≥ximo √† Terra, al√©m de informa√ß√µes sobre terremotos.
    """}]

    # Adicionar o hist√≥rico anterior ao hist√≥rico de mensagens
    for chat_turn in history: # Iterate through each chat turn
        if isinstance(chat_turn, list) and len(chat_turn) == 2: # Check if it's a valid chat turn
            user_msg, bot_msg = chat_turn # Unpack the user and bot messages
            messages.append({"role": "user", "content": user_msg})
            messages.append({"role": "assistant", "content": bot_msg})
        # Optionally, add an else block to handle unexpected history formats if needed
        # else:
        #     print(f"Skipping invalid history entry: {chat_turn}")


    # Adicionar a nova mensagem do usu√°rio
    messages.append({"role": "user", "content": message})

    # Verificar se o tema √© geologia
    if "geologia" in message.lower():
        resposta = info_geologia(message)
        # Check if resposta.matches is not empty before accessing the first element
        if resposta and resposta.get('matches'):
            geologia_info = f"Informa√ß√µes adicionais sobre geologia: {resposta['matches'][0]['metadata']['text']}"
            messages.append({"role": "system", "content": geologia_info})
        else:
             # Handle the case where no relevant geology info is found
             print("No relevant geology information found for the query.")


    # Obter a resposta do modelo
    model_response = call_groq_api(messages)

    # Se a resposta for um DataFrame, convert√™-la para texto
    if isinstance(model_response, pd.DataFrame):
        texto_corrido = ""
        for index, row in model_response.iterrows():
            texto_corrido += f"Evento {index + 1}: Magnitude {row['mag']}, Local {row['place']}, Tempo {row['time']}\n"
        model_response = texto_corrido

    # Retornar a resposta como string para Gradio
    return model_response

import gradio as gr

# Interface Gradio com ChatInterface
gr.ChatInterface(
    response,  # Fun√ß√£o que gera a resposta
    title='üåç‚òÄÔ∏èüåßÔ∏è Chat da Terra e do Universo',
    textbox=gr.Textbox(placeholder="Digite sua mensagem aqui..."),
    type='messages'
).launch(debug=True)

from transformers import pipeline
import numpy as np

transcritor = pipeline("automatic-speech-recognition",model="openai/whisper-base",generate_kwargs = {"task":"transcribe", "language":"<|pt|>"})

def transcricao(audio):
    sr, y = audio
    if y.ndim > 1:
            y = y.mean(axis=1)
    y = y.astype(np.float32)
    y /= np.max(np.abs(y))
    return transcritor({"sampling_rate": sr, "raw": y})["text"]

gr.ChatInterface(
    response,
    title='Chat da Terra e do Universo',
    textbox=gr.Textbox(placeholder="Digite sua mensagem aqui..."),
    submit_btn=gr.Button("Enviar")
)

with gr.Blocks() as demo:
    with gr.Tab("Chat da Terra e do Universo"):
        gr.ChatInterface(
            response,
            title='üåç‚òÄÔ∏èüåßÔ∏è Chat da Terra e do Universo',
            textbox=gr.Textbox(placeholder="Digite sua mensagem aqui..."),
            submit_btn=gr.Button("Enviar")

    )

    with gr.Tab("Assistente de √°udio"):
        gr.Interface(
            transcricao,
            gr.Audio(sources="microphone"),
            "text",
        )

demo.launch(debug=True)

def responde_audio(audio):
    messages = [{"role": "system", "content": """
    Voc√™ √© o Chat da Terra e do Universo e responde em portugu√™s brasileiro
    perguntas sobre a previs√£o do tempo na Terra e do espa√ßo pr√≥ximo √† Terra, al√©m de informa√ß√µes sobre terremotos.
    """}]
    message = transcricao(audio)


    # Adicionar a nova mensagem do usu√°rio
    messages.append({"role": "user", "content": message})

    # Verificar se o tema √© geologia
    if "geologia" in message.lower():
        resposta = info_geologia(message)
        geologia_info = f"Informa√ß√µes adicionais sobre geologia: {resposta.matches[0].metadata['text']}"
        messages.append({"role": "system", "content": geologia_info})

    # Obter a resposta do modelo
    model_response = call_groq_api(messages)

    # Se a resposta for um DataFrame, convert√™-la para texto
    if isinstance(model_response, pd.DataFrame):
        texto_corrido = ""
        for index, row in model_response.iterrows():
            texto_corrido += f"Evento {index + 1}: Magnitude {row['mag']}, Local {row['place']}, Tempo {row['time']}\n"
            model_response = texto_corrido

    # Retornar a resposta como string para Gradio
    return model_response

with gr.Blocks() as demo:
    with gr.Tab("Chat da Terra e do Universo"):
        gr.ChatInterface(
            response,
            title='üåç‚òÄÔ∏èüåßÔ∏è Chat da Terra e do Universo',
            textbox=gr.Textbox(placeholder="Digite sua mensagem aqui..."),
            submit_btn=gr.Button("Enviar")
            )

    with gr.Tab("Assitente de √°udio"):
        gr.Interface(
            fn=responde_audio,
            inputs=gr.Audio(sources="microphone"),
            outputs=["text"],
            title="Assistente de √Åudio"
        )

demo.launch(debug=True)

import os
from dotenv import load_dotenv
# c√≥digo omitido

load_dotenv()