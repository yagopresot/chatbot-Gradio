# -*- coding: utf-8 -*-
"""Chatbot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j5d4oRpg5EOwymhKSHiM6Qm4nCHaf_lZ
"""

!pip install litellm

from google.colab import userdata
Groq_API_Key = userdata.get('Groq_API_Key')

from litellm import completion

messages = [
    {"role": "system", "content": "Você é o chat da Terra e do Universo, que responde perguntas em português brasileiro sobre previsão do tempo na Terra e no espaço próximo à Terra, além de informações sobre terremotos."},
    {"role": "user", "content": "Qual é a frequência dos máximos solares?"}
]

response = completion(
    model = "groq/gemma2-9b-it",
    messages=messages,
    api_key=Groq_API_Key
)

print(response.choices[0].message.content)

response = completion(
    model = "groq/llama-3.3-70b-versatile",
    messages=messages,
    api_key=Groq_API_Key
)

print(response.choices[0].message.content)

def call_groq_api(messages, model="groq/llama-3.3-70b-versatile"):
  response = completion(
      model = model,
      messages = messages,
      api_key = Groq_API_Key
  )
  return response.choices[0].message.content

def chat():
  print("Iniciando chat com o modelo. Digite sair para encerrar.")
  messages = [
       {"role": "system", "content": "Você é o chat da Terra e do Universo, que responde perguntas em português brasileiro sobre previsão do tempo na Terra e no espaço próximo à Terra, além de informações sobre terremotos."},
  ]

  while True:
    user_massage = input("Você: ")
    if user_massage.lower() == "sair":
      break
    messages.append({"role": "user", "content": user_massage})
    model_response = call_groq_api(messages)
    messages.append({"role": "assistant", "content": model_response})
    print(f"Assistente: {model_response}")

chat()

import requests
import json

def previsao_do_tempo(city, country):
    Wheater_API = userdata.get('Wheater_API')
    url = f'http://api.openweathermap.org/data/2.5/weather?q={city},{country}&APPID={Wheater_API}&lang=pt_br&units=metric'
    response = requests.get(url)
    data = response.json()
    return json.dumps(data)

previsao_do_tempo('São Paulo', 'BR')

tools=[
    {
        "type": "function",
        "function": {
            "name": "previsao_do_tempo",
            "description": "Retorna a previsão do tempo para a cidade e país especificados",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "Nome da cidade"
                    },
                    "country": {
                        "type": "string",
                        "description": "Sigla do país"
                    }
                },
                "required": ["city", "country"]
            }
        }
    }
]

# Função para chamar a API com o histórico de mensagens
def call_groq_api(messages, model="groq/llama-3.3-70b-versatile"):
    global tools
    response = completion(
        model=model,
        messages=messages,
        tools=tools,
        tool_choice="auto",
        api_key=Groq_API_Key,
    )
    resposta_texto = response.choices[0].message
    chamada_ferramentas = resposta_texto.tool_calls
    if chamada_ferramentas:
      available_functions = {
        "previsao_do_tempo": previsao_do_tempo,
      }
      for tool_call in chamada_ferramentas:
        function_name = tool_call.function.name
        function_to_call = available_functions[function_name]
        function_args = json.loads(tool_call.function.arguments)
        function_response = function_to_call(
            city=function_args.get("city"),
            country=function_args.get("country"),
        )
        return function_response

    else:
      return resposta_texto.content

chat()

def verificar_tempestade_solar():
    url = "https://services.swpc.noaa.gov/products/noaa-planetary-k-index.json"
    responde = requests.get(url)
    if responde.status_code == 200: # Corrected variable name and attribute
      data = responde.json() # Corrected variable name
      latest_kp = float(data[-1][1])
      if latest_kp >= 5:
        return f"Alerta de Tempestade solar! Índice Kp atual: {latest_kp}" # Corrected variable name
      else:
        return f"Sem alertas de tempestade solar. Índice Kp atual: {latest_kp}" # Corrected variable name
    else:
      return f"Erro ao obter dados da tempestade solar. Código de status: {responde.status_code}" # Added error handling

tools = [
        {
            "type": "function",
            "function": {
                "name": "previsao_do_tempo",
                "description": "Retorna a previsão do tempo em uma cidade específica",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {
                            "type": "string",
                            "description": "Nome da cidade",
                        },
                        "country": {
                            "type": "string",
                            "description": "Sigla do país",
                        },
                    },
                    "required": ["city", "country"],
                },
            }

        },

        {
        "type": "function",
        "function": {
            "name": "verificar_tempestade_solar",
            "description": "Verifica se há uma tempestade solar em andamento",
            "parameters": {
                "type": "object",
                "properties": {},
                "required": [],
            },
        }
    }

]

# Função para chamar a API com o histórico de mensagens
def call_groq_api(messages, model="groq/llama-3.3-70b-versatile"):
    global tools
    response = completion(
        model=model,
        messages=messages,
        tools=tools,
        tool_choice="auto",
        api_key=Groq_API_Key,
    )
    resposta_texto = response.choices[0].message
    chamada_ferramentas = resposta_texto.tool_calls
    if chamada_ferramentas:
        available_functions = {
            "previsao_do_tempo": previsao_do_tempo,
            "verificar_tempestade_solar": verificar_tempestade_solar,
        }
        for tool_call in chamada_ferramentas:
            function_name = tool_call.function.name
            function_to_call = available_functions[function_name]
            function_args = json.loads(tool_call.function.arguments)

            match function_name:
                case "previsao_do_tempo":
                    function_response = function_to_call(
                        city=function_args.get("city"),
                        country=function_args.get("country"),
                    )
                case "verificar_tempestade_solar":
                    function_response = function_to_call()
            return function_response

    else:
        return resposta_texto.content

chat()

import pandas as pd

def extrair_sismos():
    url = 'https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5_day.csv'

    df = pd.read_csv(url)
    return df

extrair_sismos()

tools = [
        {
            "type": "function",
            "function": {
                "name": "previsao_do_tempo",
                "description": "Retorna a previsão do tempo em uma cidade específica",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {
                            "type": "string",
                            "description": "Nome da cidade",
                        },
                        "country": {
                            "type": "string",
                            "description": "Sigla do país",
                        },
                    },
                    "required": ["city", "country"],
                },
            }

        },

      {
        "type": "function",
        "function": {
            "name": "verificar_tempestade_solar",
            "description": "Verifica se há uma tempestade solar em andamento",
            "parameters": {
                "type": "object",
                "properties": {},
                "required": [],
            },
        }
      },
      {
        "type": "function",
        "function": {
            "name": "extrair_sismos",
            "description": "Extrai dados de sismos da USGS",
            "parameters": {
                "type": "object",
                "properties": {},
                "required": [],
            },
        }
      }
]

# Função para chamar a API com o histórico de mensagens
def call_groq_api(messages, model="groq/llama-3.3-70b-versatile"):
    global tools
    response = completion(
        model=model,
        messages=messages,
        tools=tools,
        tool_choice="auto",
        api_key=Groq_API_Key,
    )
    resposta_texto = response.choices[0].message
    chamada_ferramentas = resposta_texto.tool_calls
    if chamada_ferramentas:
      available_functions = {
        "previsao_do_tempo": previsao_do_tempo,
        "verificar_tempestade_solar": verificar_tempestade_solar,
        "extrair_sismos": extrair_sismos
      }
      for tool_call in chamada_ferramentas:
        function_name = tool_call.function.name
        function_to_call = available_functions[function_name]
        function_args = json.loads(tool_call.function.arguments)

        match function_name:
          case "previsao_do_tempo":

            function_response = function_to_call(
                city=function_args.get("city"),
                country=function_args.get("country"),
            )
          case "verificar_tempestade_solar":
            function_response = function_to_call()
          case "extrair_sismos":
            function_response = function_to_call()
        return function_response

    else:
      return resposta_texto.content

# Função para iniciar o chat, mantendo o histórico
def chat():
    print("Iniciando chat com o modelo. Digite 'sair' para encerrar.")

    # Histórico de mensagens
    messages = [{"role": "system", "content": """
    Você é o Chat da Terra e do Universo e responde em português brasileiro
    perguntas sobre a previsão do tempo na Terra e do espaço próximo à Terra, além de informações sobre terremotos.
    """}]

    while True:
        user_message = input("Você: ")
        if user_message.lower() == "sair":
            print("Encerrando chat. Até a próxima!")
            break

        # Adicionar a mensagem do usuário ao histórico
        messages.append({"role": "user", "content": user_message})

        # Chamar a API com o histórico completo
        model_response = call_groq_api(messages)

        # Exibir a resposta do assistente
        display(model_response)
        if isinstance(model_response, pd.DataFrame):
            print("O model_response é um DataFrame.")
            texto_corrido = ""
            for index, row in model_response.iterrows():
                texto_corrido += f"Evento {index + 1}: Magnitude {row['mag']}, Local {row['place']}, Tempo {row['time']}\n"

            model_response = texto_corrido


        # Adicionar a resposta do modelo ao histórico
        messages.append({"role": "assistant", "content": model_response})

chat()

!pip install pinecone

Pinecone_API = userdata.get('Pinecone_API')

from pinecone import Pinecone

pc = Pinecone(api_key=Pinecone_API)

data = [
    {
        "id": "occurrence1",
        "text": "Ouro presente em veios de quartzo em uma formação hidrotermal. Observa-se alta concentração de ouro em zonas de fraturas e falhas."
    },
    {
        "id": "occurrence2",
        "text": "Associação do ouro com sulfetos, especialmente pirita e arsenopirita, em ambiente de rochas metavulcânicas. Indica potencial para depósito orogênico de ouro."
    },
    {
        "id": "occurrence3",
        "text": "Ouro aluvial encontrado em depósitos de cascalho próximo a rios e córregos. Indica transporte e concentração secundária de ouro."
    },
    {
        "id": "occurrence4",
        "text": "Presença de ouro em formações de skarn associadas a intrusões ígneas graníticas. Indica formação relacionada a processos de metamorfismo de contato."
    },
    {
        "id": "occurrence5",
        "text": "Ouro disseminado em formações sedimentares de origem marinha, em conglomerados ricos em minerais pesados. Potencial para depósito do tipo placer ou paleoplacer."
    },
]

index = pc.Index("geologia")

embeddings = pc.inference.embed(
    "llama-text-embed-v2",
    inputs=[d['text'] for d in data],
    parameters={
        "input_type": "passage"
    }
)

vectors = []
for d, e in zip(data, embeddings):
    vectors.append({
        "id": d['id'],
        "values": e['values'],
        "metadata": {'text': d['text']}
    })

index.upsert(vectors=vectors,namespace="ns1")

query = "o ouro ocorre em sedimentos de origem marinha?"

x = pc.inference.embed(
    model="llama-text-embed-v2",
    inputs=[query],
    parameters={
        "input_type": "query"
    }
)

results = index.query(
    namespace="ns1",
    top_k=3,
    include_values=False,
    include_metadata=True,
    vector=x[0].values,
)

results

!pip install pypdf

import pypdf

from google.colab import drive
drive.mount('/content/drive')

def chunk_text(text, chunk_size=500):
    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]

with open("/content/drive/MyDrive/Projetos/dados/livro.pdf", "rb") as file:
    reader = pypdf.PdfReader(file)
    text = ""

    for page_num in range(len(reader.pages)):
        page = reader.pages[page_num]
        text += page.extract_text()

chunks = chunk_text(text)

chunks[11]

data = []

for i, chunk in enumerate(chunks[:90], start=1):
    data.append({
        "id": f"chunk{i}",
        "text": chunk.strip()
    })

for chunk in data[:5]:
    print(chunk)

embeddings = pc.inference.embed(
    "llama-text-embed-v2",
    inputs=[d['text'] for d in data],
    parameters={
        "input_type": "passage"
    }
)

vectors = []
for d, e in zip(data, embeddings):
    vectors.append({
        "id": d['id'],
        "values": e['values'],
        "metadata": {'text': d['text']}
    })

index.upsert(
    vectors=vectors,
    namespace="ns1"
)

def info_geologia(query):
    x = pc.inference.embed(
        model="llama-text-embed-v2",
        inputs=[query],
        parameters={
            "input_type": "query"
        }
    )
    results = index.query(
        namespace="ns1",
        vector=x[0].values,
        top_k=3,
        include_values=False,
        include_metadata=True
    )
    return results

query = "o ouro ocorre em sedimentos de origem marinha?"
info_geologia(query)

resposta = info_geologia(query)
resposta.matches[0].metadata['text']

# Define user_message with a placeholder value to avoid NameError
user_message = "This is a placeholder message." # Placeholder definition

# Verificar se o tema é geologia
if "geologia" in user_message.lower():
        # Chamar a função info_geologia para obter informações adicionais
        resposta = info_geologia(user_message)

# Adicionar as informações de geologia ao histórico como contexto extra
geologia_info = f"Informações adicionais sobre geologia: {resposta.matches[0].metadata['text']}"
messages.append({"role": "system", "content": geologia_info})

# Função para iniciar o chat, mantendo o histórico
def chat():
    print("Iniciando chat com o modelo. Digite 'sair' para encerrar.")

    # Histórico de mensagens
    messages = [{"role": "system", "content": """
    Você é o Chat da Terra e do Universo e responde em português brasileiro
    perguntas sobre a previsão do tempo na Terra e do espaço próximo à Terra, além de informações sobre terremotos.
    """}]

    while True:
        user_message = input("Você: ")
        if user_message.lower() == "sair":
            print("Encerrando chat. Até a próxima!")
            break

        # Adicionar a mensagem do usuário ao histórico
        messages.append({"role": "user", "content": user_message})

        # Verificar se o tema é geologia
        if "geologia" in user_message.lower():
            # Chamar a função info_geologia para obter informações adicionais
            resposta = info_geologia(user_message)

            # Adicionar as informações de geologia ao histórico como contexto extra
            geologia_info = f"Informações adicionais sobre geologia: {resposta.matches[0].metadata['text']}"
            messages.append({"role": "system", "content": geologia_info})

        # Chamar a API com o histórico completo
        model_response = call_groq_api(messages)

        # Exibir a resposta do assistente
        display(model_response)
        if isinstance(model_response, pd.DataFrame):
            print("O model_response é um DataFrame.")
            texto_corrido = ""
            for index, row in model_response.iterrows():
                texto_corrido += f"Evento {index + 1}: Magnitude {row['mag']}, Local {row['place']}, Tempo {row['time']}\n"

            model_response = texto_corrido

        # Adicionar a resposta do modelo ao histórico
        messages.append({"role": "assistant", "content": model_response})

chat()

!pip install litellm gradio pinecone

from litellm import completion
import requests
import json
import gradio as gr
import pandas as pd
from pinecone import Pinecone
import os
from google.colab import userdata

def info_geologia(query):
    PINECONE_API = userdata.get('PINECONE_API')
    pc = Pinecone(api_key=PINECONE_API)
    index = pc.Index("geologia")
    x = pc.inference.embed(
        model="llama-text-embed-v2",
        inputs=[query],
        parameters={
            "input_type": "query"
        }
    )
    results = index.query(
        namespace="ns1",
        vector=x[0].values,
        top_k=3,
        include_values=False,
        include_metadata=True
    )
    return results

def previsao_do_tempo(city, country):
    WEATHER_API = userdata.get('Wheater_API')
    url = f"http://api.openweathermap.org/data/2.5/weather?q={city},{country}&APPID={WEATHER_API}&lang=pt_br&units=metric" # Corrected variable name
    response = requests.get(url)
    data = response.json()

    return json.dumps(data)


def verificar_tempestade_solar():
    url = "https://services.swpc.noaa.gov/products/noaa-planetary-k-index.json"
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        latest_kp = float(data[-1][1])  # O último valor Kp
        if latest_kp >= 5:
            return f"Alerta de tempestade solar! Índice Kp atual: {latest_kp}"
        else:
            return f"Sem tempestade solar no momento. Índice Kp atual: {latest_kp}"
    else:
        return "Não foi possível obter informações sobre tempestades solares no momento."

def extrair_sismos():
    # Fazer a requisição para obter o conteúdo da página
    url = 'https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/4.5_day.csv'

    df = pd.read_csv(url)

    # Retornar o DataFrame com os dados
    return df


tools = [
        {
            "type": "function",
            "function": {
                "name": "previsao_do_tempo",
                "description": "Retorna a previsão do tempo em uma cidade específica",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {
                            "type": "string",
                            "description": "Nome da cidade",
                        },
                        "country": {
                            "type": "string",
                            "description": "Sigla do país",
                        },
                    },
                    "required": ["city", "country"],
                },
            }

        },

      {
        "type": "function",
        "function": {
            "name": "verificar_tempestade_solar",
            "description": "Verifica se há uma tempestade solar em andamento",
            "parameters": {
                "type": "object",
                "properties": {},
                "required": [],
            },
        }
      },
      {
        "type": "function",
        "function": {
            "name": "extrair_sismos",
            "description": "Extrai dados de sismos da USGS",
            "parameters": {
                "type": "object",
                "properties": {},
                "required": [],
            },
        }
      }
]


# Função para chamar a API com o histórico de mensagens
def call_groq_api(messages, model="groq/llama-3.3-70b-versatile"):
    global tools
    GROQ_API_KEY = userdata.get('Groq_API_Key')
    response = completion(
        model=model,
        messages=messages,
        tools=tools,
        tool_choice="auto",
        api_key=GROQ_API_KEY,
    )
    resposta_texto = response.choices[0].message
    chamada_ferramentas = resposta_texto.tool_calls
    if chamada_ferramentas:
      available_functions = {
        "previsao_do_tempo": previsao_do_tempo,
        "verificar_tempestade_solar": verificar_tempestade_solar,
        "extrair_sismos": extrair_sismos
      }
      for tool_call in chamada_ferramentas:
        function_name = tool_call.function.name
        function_to_call = available_functions[function_name]
        function_args = json.loads(tool_call.function.arguments)

        match function_name:
          case "previsao_do_tempo":

            function_response = function_to_call(
                city=function_args.get("city"),
                country=function_args.get("country"),
            )
          case "verificar_tempestade_solar":
            function_response = function_to_call()
          case "extrair_sismos":
            function_response = function_to_call()
        return function_response

    else:
      return resposta_texto.content

def response(message, history):
    messages = [{"role": "system", "content": """
    Você é o Chat da Terra e do Universo e responde em português brasileiro
    perguntas sobre a previsão do tempo na Terra e do espaço próximo à Terra, além de informações sobre terremotos.
    """}]

    # Adicionar o histórico anterior ao histórico de mensagens
    for chat_turn in history: # Iterate through each chat turn
        if isinstance(chat_turn, list) and len(chat_turn) == 2: # Check if it's a valid chat turn
            user_msg, bot_msg = chat_turn # Unpack the user and bot messages
            messages.append({"role": "user", "content": user_msg})
            messages.append({"role": "assistant", "content": bot_msg})
        # Optionally, add an else block to handle unexpected history formats if needed
        # else:
        #     print(f"Skipping invalid history entry: {chat_turn}")


    # Adicionar a nova mensagem do usuário
    messages.append({"role": "user", "content": message})

    # Verificar se o tema é geologia
    if "geologia" in message.lower():
        resposta = info_geologia(message)
        # Check if resposta.matches is not empty before accessing the first element
        if resposta and resposta.get('matches'):
            geologia_info = f"Informações adicionais sobre geologia: {resposta['matches'][0]['metadata']['text']}"
            messages.append({"role": "system", "content": geologia_info})
        else:
             # Handle the case where no relevant geology info is found
             print("No relevant geology information found for the query.")


    # Obter a resposta do modelo
    model_response = call_groq_api(messages)

    # Se a resposta for um DataFrame, convertê-la para texto
    if isinstance(model_response, pd.DataFrame):
        texto_corrido = ""
        for index, row in model_response.iterrows():
            texto_corrido += f"Evento {index + 1}: Magnitude {row['mag']}, Local {row['place']}, Tempo {row['time']}\n"
        model_response = texto_corrido

    # Retornar a resposta como string para Gradio
    return model_response

import gradio as gr

# Interface Gradio com ChatInterface
gr.ChatInterface(
    response,  # Função que gera a resposta
    title='🌍☀️🌧️ Chat da Terra e do Universo',
    textbox=gr.Textbox(placeholder="Digite sua mensagem aqui..."),
    type='messages'
).launch(debug=True)

from transformers import pipeline
import numpy as np

transcritor = pipeline("automatic-speech-recognition",model="openai/whisper-base",generate_kwargs = {"task":"transcribe", "language":"<|pt|>"})

def transcricao(audio):
    sr, y = audio
    if y.ndim > 1:
            y = y.mean(axis=1)
    y = y.astype(np.float32)
    y /= np.max(np.abs(y))
    return transcritor({"sampling_rate": sr, "raw": y})["text"]

gr.ChatInterface(
    response,
    title='Chat da Terra e do Universo',
    textbox=gr.Textbox(placeholder="Digite sua mensagem aqui..."),
    submit_btn=gr.Button("Enviar")
)

with gr.Blocks() as demo:
    with gr.Tab("Chat da Terra e do Universo"):
        gr.ChatInterface(
            response,
            title='🌍☀️🌧️ Chat da Terra e do Universo',
            textbox=gr.Textbox(placeholder="Digite sua mensagem aqui..."),
            submit_btn=gr.Button("Enviar")

    )

    with gr.Tab("Assistente de áudio"):
        gr.Interface(
            transcricao,
            gr.Audio(sources="microphone"),
            "text",
        )

demo.launch(debug=True)

def responde_audio(audio):
    messages = [{"role": "system", "content": """
    Você é o Chat da Terra e do Universo e responde em português brasileiro
    perguntas sobre a previsão do tempo na Terra e do espaço próximo à Terra, além de informações sobre terremotos.
    """}]
    message = transcricao(audio)


    # Adicionar a nova mensagem do usuário
    messages.append({"role": "user", "content": message})

    # Verificar se o tema é geologia
    if "geologia" in message.lower():
        resposta = info_geologia(message)
        geologia_info = f"Informações adicionais sobre geologia: {resposta.matches[0].metadata['text']}"
        messages.append({"role": "system", "content": geologia_info})

    # Obter a resposta do modelo
    model_response = call_groq_api(messages)

    # Se a resposta for um DataFrame, convertê-la para texto
    if isinstance(model_response, pd.DataFrame):
        texto_corrido = ""
        for index, row in model_response.iterrows():
            texto_corrido += f"Evento {index + 1}: Magnitude {row['mag']}, Local {row['place']}, Tempo {row['time']}\n"
            model_response = texto_corrido

    # Retornar a resposta como string para Gradio
    return model_response

with gr.Blocks() as demo:
    with gr.Tab("Chat da Terra e do Universo"):
        gr.ChatInterface(
            response,
            title='🌍☀️🌧️ Chat da Terra e do Universo',
            textbox=gr.Textbox(placeholder="Digite sua mensagem aqui..."),
            submit_btn=gr.Button("Enviar")
            )

    with gr.Tab("Assitente de áudio"):
        gr.Interface(
            fn=responde_audio,
            inputs=gr.Audio(sources="microphone"),
            outputs=["text"],
            title="Assistente de Áudio"
        )

demo.launch(debug=True)

import os
from dotenv import load_dotenv
# código omitido

load_dotenv()